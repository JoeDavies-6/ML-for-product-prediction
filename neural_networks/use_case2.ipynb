{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7911,"status":"error","timestamp":1659427268077,"user":{"displayName":"Joseph Davies","userId":"13164895450762873746"},"user_tz":-60},"id":"B1T4pcokB8Rn","colab":{"base_uri":"https://localhost:8080/","height":356},"outputId":"928d9e4d-455e-4f0f-fd9b-845964ff6335"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-86bf0f1a6cf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msmogn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTENC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'smogn'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# Import required packages\n","import os\n","import datetime\n","import sys\n","import IPython\n","import IPython.display\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import tensorflow as tf\n","from keras.preprocessing.sequence import TimeseriesGenerator  \n","from keras.models import Sequential\n","from sklearn.linear_model import LinearRegression\n","from keras.layers import Dense, LSTM, Dropout\n","from sklearn.preprocessing import MinMaxScaler\n","from statistics import mean\n","from ast import literal_eval\n","from scipy.stats import sem\n","from matplotlib import container\n","import smogn\n","from imblearn.over_sampling import SMOTENC"]},{"cell_type":"code","source":["os.getcwd()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"RsHdhU3_lVOy","executionInfo":{"status":"ok","timestamp":1659427447791,"user_tz":-60,"elapsed":323,"user":{"displayName":"Joseph Davies","userId":"13164895450762873746"}},"outputId":"d7c730df-69d8-4203-dd55-1190dc5ebaaa"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4IJmgfv3I3Ac"},"outputs":[],"source":["\"\"\"\n","The code in this cell uses the functions defined below to test use case 2. \n","This is done by oversampling and settings run 17 and 25 as the test runs\n","\"\"\"\n","# Load the dataframe from the csv file\n","dir_path = os.getcwd()\n","df = open_dataframe(dir_path + \"data/data.csv\")\n","features =  ['blue_change', 'saturation_change', 'green_change', 'pressureE_change', 'hue_change',\n","              'luminance_percent_change', 'humidity_change', 'temperature', 'hydrazine_batches',\n","              'yeo-johnson_green_cumulative', 'best_temperature_std_dev_cumulative']\n","new_runs = ['Crocus-017', 'Crocus-025']\n","historic_runs = ['Crocus-014', 'Crocus-015', 'Crocus-016', 'Crocus-018', 'Crocus-019', 'Crocus-020',\n","     'Crocus-022', 'Crocus-023']\n","# Apply smogn to over/under sample the training runs\n","training_runs = training_sampler(df.loc[historic_runs], historic_runs)\n","run_labels = list(label_maker(training_runs))\n","# Initialise the variables that control the time ahead that is predicted, window size, and downsample rate\n","dir_used = 'use_case2/'\n","time_ahead = [1800, 3600, 7200]\n","time_used = 7200\n","downsample_rate = 90\n","\n","dir_check = dir_path + dir_used\n","if not os.path.exists(dir_check):\n","  os.makedirs(dir_check)\n","make_oversample_df(df, training_runs, new_runs, run_labels, dir_used)\n","df_filepath = os.path.join(dir_path, dir_used, \"dataframe.csv\")\n","df = open_dataframe(path=df_filepath)\n"," # Start the pipeline used to predict product formation\n","a = startPipeline(dir_path, df, new_runs, run_labels, features, time_ahead, time_used, downsample_rate, dir_used, repeats=10)\n","a.pipeline()\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFEGGL-SAZXO"},"outputs":[],"source":["class startPipeline:\n","  \"\"\"This object calls and links the two main predictive objects, predictValues and predictAhead\"\"\"\n","  def __init__(self, dir_path, df, new_runs, historic_runs, features, time_ahead, time_used, downsample_rate, dir_used, repeats=1):\n","    self.dir_path = dir_path\n","    self.df = df\n","    self.crocus_ls = sorted(list(set(df.index)))\n","    self.new_runs = new_runs\n","    self.historic_runs = historic_runs\n","    self.features = features\n","    self.time_ahead = time_ahead\n","    self.time_used = time_used\n","    self.downsample_rate = downsample_rate\n","    self.base_dir = os.path.join((dir_path + dir_used))\n","    self.repeats = repeats\n","    self.dir_used = dir_used\n","\n","\n","  def pipeline(self):\n","    \"\"\" This gets the predicted values for the instantaneous results and saves them into a file\n","    Predicted values are obtained for all of the runs, including the training data.\n","    These predicted values are then used in part to the train the next model\n","    \"\"\"\n","    runs_ls = self.historic_runs + self.new_runs \n","    #  Initialise the predictValues object for later use\n","    self.a = predictValues(self.df, ['empty'], ['empty'], self.features, self.base_dir, runs_ls)\n","    # Make a list of which runs values have already been predicted\n","    if os.path.isfile(self.base_dir + \"predictions_lstm.csv\"):\n","      preds_df = pd.read_csv(self.base_dir + \"predictions_lstm.csv\", index_col='crocus_id')\n","      finished_id_ls = sorted(list(set(preds_df.index)))\n","    else:\n","      finished_id_ls = []\n","      \n","    for run in runs_ls:\n","      if run in finished_id_ls:\n","        # If a run has already been predicted, then skip\n","        continue\n","      # Remove the current test run from the training set\n","      training_runs = [x for x in self.historic_runs if x.split('_')[0] != run.split('_')[0]]\n","      # Call the predict values object again and make product predictions - these are saved and assigned later\n","      self.a = predictValues(self.df, run, training_runs, self.features, self.base_dir, runs_ls)\n","      self.a.get_predictions()\n","    # Add the predictions column to the dataframe\n","    runs_df = self.df.loc[runs_ls]\n","    runs_df['predictions'] = self.a.make_predictions_dataframe()\n","    self.features.append('predictions')\n","\n","    for new_run in self.new_runs:\n","      # get mean absolute error for the instantaneous predictions\n","      mae = error_at_t_zero(self.dir_used, new_run)\n","      # save this value to results file\n","      new_df = pd.DataFrame({'new_runs': [new_run], 'historic_runs': [self.historic_runs], 'features': 'na', 'time_ahead': 0, 'time_used': 'na', 'downsample_rate': 'na', 'predictions': 'na', 'mean_err': mae, 'errors': 'na'})\n","      new_df.to_csv(self.base_dir + new_run + 'results0.csv')\n","      # Iterate through each time in the list of time aheads to predict\n","      for time_ahead in self.time_ahead:\n","        df_ls = []\n","        # Repeat according to specified number of repeats\n","        for i in range(self.repeats):\n","          # Make and return the ahead predictions and their mean absolute errors\n","          ahead_predictions, mean_err = predictAhead(runs_df, self.time_used, time_ahead, self.downsample_rate, self.historic_runs, [new_run], self.features, self.base_dir).get_predictions()\n","          new_df = pd.DataFrame({'new_runs': [new_run], 'historic_runs': [self.historic_runs], 'features': [self.features], 'time_ahead': [time_ahead], 'time_used': [self.time_used], 'downsample_rate': [self.downsample_rate], 'predictions':[ahead_predictions], 'mean_err': mean_err})\n","          df_ls.append(new_df)\n","        # concatenate and save the results\n","        new_df = pd.concat(df_ls)\n","        new_df.to_csv(self.base_dir + new_run + 'results'+ str(time_ahead) +'.csv')\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ZO1VTuCAceQ"},"outputs":[],"source":["class predictValues:\n","  \"\"\"This class contains all variables and functions used to predict the current product\"\"\"\n","  def __init__(self, df, pred_run, train_runs, features, dir_base, all_runs):\n","    self.df = df\n","    self.crocus_ls = sorted(list(set(df.index)))\n","    self.pred_run = pred_run\n","    self.train_runs = train_runs\n","    self.runs_ls =  self.train_runs + [self.pred_run]\n","    self.features = features\n","    self.base_dir = dir_base\n","    self.all_runs = all_runs\n","\n","  def get_predictions(self):\n","    \"\"\"This function calls all other functions in the correct order\"\"\"\n","    self.test_train_split()\n","    self.linear_regression()\n","    self.make_lstm_sequence()\n","    self.train_lstm()\n","    self.evaluate_model()\n","    \n","\n","  def test_train_split(self):\n","    # split the test and train data\n","    df_grouped = df.groupby(\"crocus_id\", sort=\"datetime\")\n","    self.training_dfs = [x for _, x in df_grouped]\n","    self.train_df = self.df.loc[self.train_runs]\n","    self.test_df = self.df.loc[self.pred_run]\n","    # scale the x data\n","    # get x and y\n","    self.x_train = self.train_df[self.features]\n","    self.y_train = np.array(self.train_df['Product'])\n","    self.x_test = self.test_df[self.features]\n","    self.y_test = np.array(self.test_df['Product'])\n","    ss = MinMaxScaler()\n","    self.x_train_scaled = np.array(ss.fit_transform(self.x_train))\n","    self.x_test_scaled = np.array(ss.transform(self.x_test))\n","\n","  def linear_regression(self):\n","    # train model and predict values for the first 20 minutes because the NN requires 20 minutes of data, linear regression is used for this time\n","    regressor = LinearRegression(fit_intercept=True)\n","    regressor = LinearRegression(fit_intercept=True)\n","    regressor.fit(self.x_train_scaled, self.y_train)\n","    self.y_pred = regressor.predict(self.x_test_scaled)\n","    # save predictions, making a new file if one doesn't currently exist\n","    pred_file = os.path.join(self.base_dir + 'predictions_lr.csv')\n","    y_preds_len = len(self.y_pred)\n","    croc_id_arr = [self.pred_run] * y_preds_len\n","    if os.path.isfile(pred_file):\n","      preds_df = pd.read_csv(self.base_dir + 'predictions_lr.csv', index_col=0)\n","      time_val = self.test_df['time'].reset_index(drop=True).values\n","      new_df = pd.DataFrame({'crocus_id': croc_id_arr, 'time': time_val, 'predictions': self.y_pred})\n","      save_df = pd.concat([preds_df, new_df])\n","      save_df.to_csv(self.base_dir + 'predictions_lr.csv')\n","    else:\n","      time_val = self.test_df['time'].reset_index(drop=True).values\n","      new_df = pd.DataFrame({'crocus_id': croc_id_arr, 'time': time_val, 'predictions':self.y_pred})\n","      new_df.to_csv(self.base_dir + 'predictions_lr.csv')\n","\n","  def make_lstm_sequence(self):\n","    # This scales and transforms the data and calls the dequence creator function to make a sliding window of data\n","    sc = MinMaxScaler(feature_range=(0, 1))\n","    sc.fit(self.x_train) \n","    X_train = []\n","    y_train = []\n","    self.lookback_window_samples = 20 * 6\n","    self.lookahead_label_offset = 0 * 6\n","    X_test, y_test = sequence_creator(self.test_df, self.lookback_window_samples, sc, self.features, self.lookahead_label_offset, )\n","    self.x_test, self.y_test = np.array(X_test), np.array(y_test)\n","    for single_training_run in self.training_dfs:\n","      xt, yt = sequence_creator(single_training_run, self.lookback_window_samples, sc,  self.features, self.lookahead_label_offset)\n","      X_train.extend(xt)\n","      y_train.extend(yt)\n","\n","    self.X_train = np.asarray(X_train)\n","    self.y_train = np.asarray(y_train)\n","\n","  def train_lstm(self):\n","    # The LSTM is trained\n","    final_loss = 1000000\n","    # The LSTM will repeat the training process if the final loss is higher than expected\n","    while final_loss > 10:\n","      self.regressor = Sequential()\n","      self.regressor.add(\n","        LSTM(50, activation='relu', return_sequences=True, input_shape=(self.lookback_window_samples, self.x_train.shape[1])))\n","      self.regressor.add(LSTM(50, activation='relu', return_sequences=True))\n","      self.regressor.add(LSTM(50, activation='sigmoid', return_sequences=False))\n","      self.regressor.add(Dense(50))\n","      self.regressor.add(Dropout(0.2))\n","      self.regressor.add(Dense(1))\n","      self.regressor.compile(optimizer='adam', loss='mean_squared_error')\n","      self.regressor.fit(self.X_train, self.y_train, epochs=10, batch_size=128, shuffle=True)\n","      loss_per_epoch = self.regressor.history.history['loss']\n","      final_loss = loss_per_epoch[-1]\n","      if np.isnan(final_loss):\n","        final_loss = 10000\n","\n","  def evaluate_model(self):\n","    # The trained model is evaluated by making predictions and comparing to the true yield to compute the MAE\n","    self.predicted_yield = self.regressor.predict(self.x_test).reshape(-1)\n","    predicted_yield = self.predicted_yield\n","    pred_file = os.path.join(self.base_dir + 'predictions_lstm.csv')\n","    y_preds_len = len(predicted_yield)\n","    croc_id_arr = [self.pred_run] * y_preds_len\n","    time_val = self.test_df['time'][self.lookback_window_samples:].reset_index(drop=True).values\n","    # calculate errors and save to predictions lstm csv\n","    mae = np.mean(np.abs(predicted_yield - self.y_test))\n","    \n","    if os.path.isfile(pred_file):\n","      preds_df = pd.read_csv(self.base_dir + 'predictions_lstm.csv', index_col=0)\n","      new_df = pd.DataFrame({'crocus_id': croc_id_arr, 'time': time_val, 'predictions': self.predicted_yield})\n","      save_df = pd.concat([preds_df, new_df])\n","      save_df.to_csv(self.base_dir + 'predictions_lstm.csv')\n","    else:\n","      new_df = pd.DataFrame({'crocus_id': croc_id_arr, 'time': time_val, 'predictions': self.predicted_yield})\n","      new_df.to_csv(self.base_dir + 'predictions_lstm.csv')\n","    self.mae = mae\n","   \n","  def make_predictions_dataframe(self):\n","    # returns all of the instantaneous predicted product values\n","    preds_ls = []\n","    lr_infile = os.path.join(self.base_dir + 'predictions_lr.csv')\n","    lr_df = pd.read_csv(lr_infile, header=0, index_col='crocus_id')\n","    lstm_infile = os.path.join(self.base_dir + 'predictions_lstm.csv')\n","    lstm_df = pd.read_csv(lstm_infile, header=0, index_col='crocus_id')\n","    df_ls = []\n","\n","    for run in sorted(self.all_runs):\n","      preds_t_ls = []\n","      preds_t_ls.extend(lr_df.loc[run]['predictions'][0:120])\n","      preds_t_ls.extend(lstm_df.loc[run]['predictions'])\n","      croc_id_arr = [run] * len(preds_t_ls)\n","      df_t = pd.DataFrame({'crocus_id': croc_id_arr, 'predictions': preds_t_ls})\n","      df_ls.append(df_t)\n","\n","    save_df = pd.concat(df_ls)\n","    save_df.to_csv(self.base_dir + 'all_predictions.csv')\n","    return save_df['predictions'].values\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0kq-4T1Xhze7"},"outputs":[],"source":["class predictAhead:\n","  \"\"\"\n","  This class contains all the functions and variables needed to predict future product values\n","  t1 is the time to be used, t2 is the ahead to predict and t3 is the time between instances\n","  \"\"\"\n","  def __init__(self, df, t1:int, t2:int, t3:int, training_runs:list, test_runs:list, features:list, base_dir):\n","    self.df = df\n","    self.t1 = t1  # window size\n","    self.t2 = t2  # time ahread to predict\n","    self.t3 = t3  # downsample rate\n","    self.training_runs = training_runs\n","    self.test_runs = test_runs\n","    self.features = features\n","    self.crocus_ls = sorted(list(set(df.index)))\n","    self.base_dir = base_dir\n","\n","  def get_predictions(self):\n","    \"\"\"This function calls all other functions in the correct order\"\"\"\n","    preds_df = pd.read_csv(self.base_dir + \"all_predictions.csv\", index_col='crocus_id')\n","    # skips if predictions have already been made\n","    if self.test_runs in sorted(list(set(preds_df.index))):\n","      return None, None  \n","    self.prepare_dataframe()\n","    self.split_windows()\n","    self.fix_arrays()\n","    self.make_model()\n","    self.fit_model()\n","    self.evaluate_model()\n","    return self.preds, self.mean_err\n","\n","  def prepare_dataframe(self):\n","   # Downsamples the data - makes the window of data to be used for predictions and then predict x time ahead\n","    df_ls = []\n","    df = self.df\n","    # Downsamples the data\n","    df = df.iloc[::int(self.t3/10), :]\n","    for run in self.crocus_ls:\n","      df.loc[run]['time'] = df.loc[run]['time'] - df.loc[run]['time'][0]\n","    # Make all long runs start at x time\n","    df_ls = []\n","    run_ls = []\n","    for run in self.crocus_ls:\n","      if run in ['Crocus-023', 'Crocus-025']:\n","        df_t = df.loc[run][df.loc[run]['time'] > 7200]\n","        df_ls.append(df_t)\n","        run_ls.append({'run': run, 'length': len(df_t)})\n","      else:\n","        df_t = df.loc[run]\n","        df_ls.append(df_t)\n","        run_ls.append({'run': run, 'length': len(df_t)})\n","\n","    df = pd.concat(df_ls)\n","    df_run = pd.DataFrame(run_ls)\n","    shortest_length = int(df_run['length'].min())\n","    # Make all runs the same length\n","    df_ls = []\n","    for run in self.crocus_ls:\n","      df_t = df.loc[run][0:shortest_length]\n","      df_ls.append(df_t)\n","    df = pd.concat(df_ls)\n","    self.df = df\n","\n","  def split_windows(self):\n","    # Test train split of dataframes and make sliding window\n","    self.train_df = self.df.loc[self.training_runs]\n","    self.test_df = self.df.loc[self.test_runs]\n","    self.x_train, self.y_train = [], []\n","    self.train_crocus_ls = sorted(list(set(self.train_df.index)))\n","    seq_length = int(self.t1 / self.t3 - 1)  # instances to use\n","    ahead = int(self.t2 / self.t3) # how many instances ahead to predict\n","    for run in self.training_runs:\n","      for i in range(len(self.train_df.loc[run]) - seq_length - ahead -1):\n","        data_train = np.array(self.train_df.loc[run][self.features][i:i+seq_length])\n","        self.x_train.append(data_train)\n","        self.y_train.append(self.train_df.loc[run]['Product'][i+ahead:i+seq_length+ahead])\n","\n","    self.x_test, self.y_test = [], []\n","    test_crocus_ls = sorted(list(set(self.test_df.index)))\n","    for run in self.test_runs:\n","      for i in range(len(self.test_df.loc[run]) - seq_length - ahead - 1):\n","        data_test = np.array(self.test_df.loc[run][self.features][i:i+seq_length])\n","        self.x_test.append(data_test)\n","        self.y_test.append(self.test_df.loc[run]['Product'][i+ahead:i+seq_length+ahead])\n","\n","  def fix_arrays(self):\n","    # Scale and reshape into arrays suitable for input into the ML model\n","    seq_length = int(self.t1 / self.t3 - 1) \n","    x_train, y_train = np.array(self.x_train), np.array(self.y_train)\n","    x_test, y_test = np.array(self.x_test), np.array(self.y_test)\n","    # scale\n","    x_scaler = MinMaxScaler()\n","    x_train = x_scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n","    x_test = x_scaler.transform(x_test.reshape(x_test.shape[0], -1))\n","    # reshape\n","    self.x_train = x_train.reshape(x_train.shape[0], seq_length, len(self.features))\n","    self.y_train = y_train.reshape(y_train.shape[0], y_train.shape[1], 1)\n","    self.x_test = x_test.reshape(x_test.shape[0], seq_length, len(self.features))\n","    self.y_test = y_test.reshape(y_test.shape[0], y_test.shape[1], 1)\n","\n","  def make_model(self):\n","    # Define and train model\n","    final_loss = 1000000\n","    # The LSTM will repeat the training process if the final loss is higher than expected\n","    while final_loss > 10:\n","      self.model = Sequential()\n","      self.model.add(\n","        LSTM(50, activation='relu', return_sequences=True, input_shape=(int(self.t1 / self.t3 - 1) , len(self.features))))\n","      self.model.add(LSTM(50, activation='relu', return_sequences=True))\n","      self.model.add(LSTM(50, activation='sigmoid', return_sequences=False))\n","      self.model.add(Dense(50))\n","      self.model.add(Dropout(0.2))\n","      self.model.add(Dense(1))\n","      self.model.compile(optimizer='adam', loss='mean_squared_error')\n","      self.model.summary()\n","      self.model.fit(self.x_train, self.y_train, epochs=20, verbose=True)\n","      loss_per_epoch = self.model.history.history['loss']\n","      final_loss = loss_per_epoch[-1]\n","      if np.isnan(final_loss):\n","        final_loss = 10000\n","\n","  def evaluate_model(self):\n","    \"\"\"Make predictions and evaluate the model by mean absolute error\"\"\"\n","    preds = self.model.predict(self.x_test)\n","    self.preds = preds\n","    actuals = self.y_test\n","    error_ls1 = []\n","    for idx, (pred, actual) in enumerate(zip(preds, actuals)):\n","      error = actual[-1] - pred\n","      error_ls1.append(float(abs(error)))\n","    self.mean_err = mean(error_ls1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nz32WBUGCB7H"},"outputs":[],"source":["def open_dataframe(infile):\n","  # Load df from file\n","  in_file = os.path.join(infile)\n","  df = pd.read_csv(in_file, header=0, index_col='crocus_id', parse_dates=[\"datetime\"])\n","  return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"beOn-SItE0no"},"outputs":[],"source":["def sequence_creator(xd, sample_lookback, sc, features, label_lookahead=0):\n","\tx_runs = list()\n","\ty_runs = list()\n","\tfor i in range(sample_lookback, len(xd) - label_lookahead):\n","\t\tx_runs.append(sc.transform(xd.iloc[i - sample_lookback:i][features]))\n","\t\ty_runs.append(xd.iloc[i + label_lookahead][\"Product\"])\n","\treturn x_runs, y_runs\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3132,"status":"ok","timestamp":1646066069481,"user":{"displayName":"Joseph Davies","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13164895450762873746"},"user_tz":0},"id":"McOKVZ3t4txO","outputId":"5a3bd210-35f2-43b0-e8eb-024a2f8e2025"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Errors for Crocus-025 in 25_test/: \n"," LSTM: 0.670349974186891 \n"," Linear regression: 3.2661723757158936 \n"," Overall: 0.776699816516519 \n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["0.776699816516519"]},"metadata":{},"execution_count":12}],"source":["def error_at_t_zero(path_dir, base_dir, run):\n","  \"\"\"Get error for predictions at t=0\"\"\"\n","  # open file\n","  run = run.split('_')[0]\n","  filename = os.path.join(path_dir + base_dir + \"all_predictions.csv\")\n","  pred_df = pd.read_csv(filename, index_col=1)\n","  pred_run_df = pred_df.loc[run]['predictions']\n","  # open dataframe\n","  df = open_dataframe()\n","  run_df = df.loc[run]['Product']\n","  # lstm errors\n","  lstm_mae = mean(abs(pred_run_df.iloc[120:] - run_df.iloc[120:]))\n","  # lr errors\n","  lr_mae = mean(abs(pred_run_df.iloc[:120] - run_df.iloc[:120]))\n","  # overall errors\n","  mae = mean(abs(pred_run_df - run_df))\n","  return mae\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OeLKxxmfvhwc"},"outputs":[],"source":["def label_maker(training_runs):\n","  # Label training runs selected for oversampling\n","  seen = {}\n","  for x in training_runs:\n","      if x in seen:\n","          seen[x] += 1\n","          yield \"%s_%d\" % (x, seen[x])\n","      else:\n","          seen[x] = 0\n","          yield x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0x2PDKrtR7ND"},"outputs":[],"source":["def training_sampler(df, historic_runs):\n","  # Select runs for over/undersampling using smogn and relevant training data\n","  dic_ls = []\n","  for run in historic_runs:\n","    run_df = df.loc[run]\n","    final_product = run_df['Product'].iloc[-1]\n","    start_product = run_df['Product'].iloc[0]\n","    final_green = run_df['green_cumulative'].iloc[-1]\n","    dic_ls.append({'crocus_id': run, 'Product': final_product, 'Start_product': start_product, 'green': final_green})\n","  new_df = pd.DataFrame(dic_ls)\n","  smogn_output = smogn.smoter(\n","      data=new_df,\n","      y='Product',\n","      rel_thres=0.1,\n","      rel_coef=0.75,\n","      # samp_method='balanced',\n","      rel_xtrm_type='low',\n","      rel_method='auto',\n","      replace=True\n","  )  \n","  return list(sorted(smogn_output['crocus_id']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kkprt_b69oKP"},"outputs":[],"source":["def make_oversample_df(df, infile, training_runs, new_runs, run_labels, dir):\n","  # Make the new dataframe to enable over/undersampling of data\n","  df_ls = []\n","  for run, label in zip(training_runs, run_labels):\n","    run_df = df.loc[run]\n","    run_df.index =  [label] * len(run_df)\n","    df_ls.append(run_df)\n","  for run in new_runs:\n","    run_df = df.loc[run]\n","    run_df.index = [run] * len(run_df)\n","    df_ls.append(run_df)\n","  new_df = pd.concat(df_ls)\n","  df_filepath = os.path.join(infile, dir_used, \"dataframe.csv\")\n","  new_df.index.rename('crocus_id', inplace=True)\n","  new_df.to_csv(df_filepath)\n","\n"]}],"metadata":{"colab":{"name":"use_case2.ipynb","provenance":[{"file_id":"10UTSphUMWwpKCDFWdlLnvMLkRnU1t4ws","timestamp":1639402040501}],"authorship_tag":"ABX9TyMIi2urnqR9ODmK2YTttK/7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}